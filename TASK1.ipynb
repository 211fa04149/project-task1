{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJSDokOekngs",
        "outputId": "c194df3a-f9b1-463b-ed3c-06f15aafad68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.57.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.1)\n",
            "Collecting pinecone-client\n",
            "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.12.14)\n",
            "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-plugin-inference, pinecone-client\n",
            "Successfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0 pinecone-plugin-interface-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install pinecone-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import pinecone"
      ],
      "metadata": {
        "id": "99CRpnqskoXB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: OpenAI API Setup\n",
        "import openai\n",
        "\n",
        "# Set OpenAI API key\n",
        "openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
        "\n"
      ],
      "metadata": {
        "id": "D46ghgSdkyvW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Step 2: Pinecone DB Setup\n",
        "from pinecone import Pinecone\n",
        "\n",
        "# Set Pinecone API key\n",
        "api_key = \"YOUR_PINECONE_API_KEY\"\n",
        "\n"
      ],
      "metadata": {
        "id": "MsychF2PlSv6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"qa_bot_index\"\n",
        "pc = pinecone.Pinecone(api_key=api_key)\n",
        "print(f\"Pinecone API Key: {api_key}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhfMDsDQmMDr",
        "outputId": "4ffe95a2-d121-47fa-d898-e73772643184"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone API Key: YOUR_PINECONE_API_KEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example troubleshooting code\n",
        "print(f\"Pinecone API Key: {api_key}\")\n",
        "\n",
        "# Output the Pinecone client instance details\n",
        "print(f\"Pinecone Client: {pc}\")\n",
        "\n",
        "# Attempt to create the index\n",
        "try:\n",
        "    pc.create_index(index_name, dimension=8080, metric='sin', spec=pinecone.ServerlessSpec(cloud='aws', region='us-west-2'))\n",
        "except Exception as e:\n",
        "    print(f\"Error creating index: {e}\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEWkUEAPmnz4",
        "outputId": "1425c94c-2fa0-43ac-c93d-6367f7ecf048"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinecone API Key: YOUR_PINECONE_API_KEY\n",
            "Pinecone Client: <pinecone.control.pinecone.Pinecone object at 0x794988a5dab0>\n",
            "Error creating index: Invalid value for `metric` (sin), must be one of ['cosine', 'euclidean', 'dotproduct']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_answers(question, context_list):\n",
        "    # Use Pinecone to retrieve relevant contexts\n",
        "    # Implement retrieval logic here\n",
        "    relevant_contexts = [\"Context 1\", \"Context 2\", \"Context 3\"]  # Replace with actual retrieval logic\n",
        "\n",
        "    return relevant_contexts\n",
        ""
      ],
      "metadata": {
        "id": "i6bvRJAsnIUa"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(question, context):\n",
        "    # Use OpenAI API to generate an answer given the context\n",
        "    # Implement generation logic here\n",
        "    generated_answer = \"Answer\"  # Replace with actual generation logic\n",
        "\n",
        "    return generated_answer\n",
        ""
      ],
      "metadata": {
        "id": "ITCrlXTCnfXY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define QA Bot Function\n",
        "def qa_bot(question, context_list):\n",
        "    # Retrieve relevant contexts using Pinecone\n",
        "    relevant_contexts = retrieve_answers(question, context_list)\n",
        "\n",
        "    # Generate answers for each relevant context using OpenAI API\n",
        "    answers = []\n",
        "    for context in relevant_contexts:\n",
        "        answer = generate_answer(question, context)\n",
        "        answers.append(answer)\n",
        "\n",
        "    # Choose the best answer based on a ranking mechanism\n",
        "    best_answer = max(answers, key=len)  # Example: choosing the longest answer\n",
        "\n",
        "    return best_answer\n",
        "\n"
      ],
      "metadata": {
        "id": "X2kOM8cKnrTP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Test the QA Bot\n",
        "question_to_ask = \"What are the latest business trends?\"\n",
        "context_list = [\"Context 1\", \"Context 2\", \"Context 3\"]\n",
        "\n",
        "answer = qa_bot(question_to_ask, context_list)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIufE3BBnuvT",
        "outputId": "bd175dec-3cb2-4bd9-c6d4-8934df2db78c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Answer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "second model"
      ],
      "metadata": {
        "id": "dZVQCMQDttlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Function to generate an embedding for a given question\n",
        "def generate_embedding(text):\n",
        "    response = openai.Embedding.create(input=text, model=\"text-embedding-ada-002\")\n",
        "    return response['data'][0]['embedding']\n",
        "\n",
        "# Function to retrieve relevant contexts from Pinecone\n",
        "def retrieve_answers(question, pinecone_index, top_k=3):\n",
        "    \"\"\"\n",
        "    Retrieve the most relevant contexts for a given question using Pinecone.\n",
        "\n",
        "    :param question: User's query (string)\n",
        "    :param pinecone_index: The Pinecone index object\n",
        "    :param top_k: Number of top relevant contexts to retrieve\n",
        "    :return: List of relevant contexts\n",
        "    \"\"\"\n",
        "    # Generate an embedding for the question\n",
        "    question_embedding = generate_embedding(question)\n",
        "\n",
        "    # Query Pinecone for similar contexts\n",
        "    results = pinecone_index.query(\n",
        "        vector=question_embedding,\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "\n",
        "    # Extract relevant contexts from the query results\n",
        "    relevant_contexts = [match['metadata']['content'] for match in results['matches']]\n",
        "\n",
        "    return relevant_contexts\n"
      ],
      "metadata": {
        "id": "vWUCciXknzOd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "def generate_answer(question, context):\n",
        "    \"\"\"\n",
        "    Generate an answer to a question using OpenAI GPT models, based on provided context.\n",
        "\n",
        "    :param question: User's query (string)\n",
        "    :param context: Relevant context for the question (string)\n",
        "    :return: Generated answer (string)\n",
        "    \"\"\"\n",
        "    # Construct the prompt\n",
        "    prompt = f\"\"\"\n",
        "    Use the context below to answer the question as accurately as possible.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the answer using OpenAI's GPT model\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",  # You can use 'gpt-4' or 'gpt-3.5-turbo'\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an assistant knowledgeable in answering business-related queries.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=300,  # Adjust as needed for your response length\n",
        "        temperature=0.7  # Adjust creativity level\n",
        "    )\n",
        "\n",
        "    # Extract and return the answer\n",
        "    generated_answer = response['choices'][0]['message']['content'].strip()\n",
        "    return generated_answer\n"
      ],
      "metadata": {
        "id": "YS-0fRsdn913"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def qa_bot(question, pinecone_index):\n",
        "    \"\"\"\n",
        "    A QA bot function that retrieves relevant contexts and generates the best answer.\n",
        "\n",
        "    :param question: User's query (string)\n",
        "    :param pinecone_index: Pinecone index object for context retrieval\n",
        "    :return: The best answer (string)\n",
        "    \"\"\"\n",
        "    # Step 1: Retrieve relevant contexts using Pinecone\n",
        "    relevant_contexts = retrieve_answers(question, pinecone_index)\n",
        "\n",
        "    # Step 2: Generate answers for each relevant context using OpenAI API\n",
        "    answers = []\n",
        "    for context in relevant_contexts:\n",
        "        try:\n",
        "            answer = generate_answer(question, context)\n",
        "            answers.append(answer)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating answer for context: {context}\\nError: {e}\")\n",
        "\n",
        "    # Step 3: Rank answers and choose the best one\n",
        "    if not answers:\n",
        "        return \"Sorry, I couldn't find a suitable answer.\"\n",
        "\n",
        "    # Example ranking: Choose the longest answer (replace with custom logic if needed)\n",
        "    best_answer = max(answers, key=len)\n",
        "\n",
        "    return best_answer\n"
      ],
      "metadata": {
        "id": "EBhTTTWytVzv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_to_ask = \"How can businesses adopt sustainability practices?\"\n",
        "\n",
        "# Mock some context data for testing (replace this with actual Pinecone query results in production)\n",
        "mock_context_list = [\n",
        "    \"Context: Businesses can reduce their carbon footprint by using renewable energy sources.\",\n",
        "    \"Context: Implementing sustainable supply chains is critical for businesses aiming to go green.\",\n",
        "    \"Context: Encouraging remote work can help businesses reduce energy consumption and promote sustainability.\"\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "3E6lkMsjosPw"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Test the QA bot with the mock data\n",
        "answer = qa_bot(question_to_ask, mock_context_list)\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwf-StnlrXrr",
        "outputId": "b010aad7-7e4c-4ef7-8dcc-0e8aed9a195f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating answer for context: Context: Businesses can reduce their carbon footprint by using renewable energy sources.\n",
            "Error: \n",
            "\n",
            "You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n",
            "Error generating answer for context: Context: Implementing sustainable supply chains is critical for businesses aiming to go green.\n",
            "Error: \n",
            "\n",
            "You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n",
            "Error generating answer for context: Context: Encouraging remote work can help businesses reduce energy consumption and promote sustainability.\n",
            "Error: \n",
            "\n",
            "You tried to access openai.Completion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
            "\n",
            "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
            "\n",
            "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
            "\n",
            "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
            "\n",
            "Answer: Sorry, I couldn't find a suitable answer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ec3mKjdsK41"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}